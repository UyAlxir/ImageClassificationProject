** args :
do_train  :  True
do_valid  :  True
do_test  :  True
model  :  FinalNet
model_path  :  ../saved_model/
datasets_path  :  ../datasets/
dataset  :  CIFAR100
train_batch_size  :  32
test_batch_size  :  32
epoch  :  10
with_cuda  :  True
lr  :  0.03
log_path  :  ../log/
device  :  cuda:0
** args 


FinalNet(
  (convs): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))
    (1): ReLU()
    (2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))
    (4): ReLU()
    (5): AvgPool2d(kernel_size=(3, 3), stride=1, padding=1)
    (6): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))
    (7): ReLU()
    (8): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (Flatten): Flatten()
  (Linear): Sequential(
    (0): Dropout(p=0.2, inplace=False)
    (1): Linear(in_features=4096, out_features=100, bias=True)
  )
)

** training is finished **
** the classification report of current trained model on the validation dataset is :
              precision    recall  f1-score   support

           0       0.78      0.43      0.55        42
           1       0.49      0.57      0.53        47
           2       0.24      0.29      0.26        48
           3       0.31      0.18      0.23        55
           4       0.24      0.20      0.22        46
           5       0.32      0.53      0.40        53
           6       0.56      0.41      0.47        44
           7       0.48      0.57      0.52        53
           8       0.37      0.74      0.49        35
           9       0.53      0.50      0.51        54
          10       0.50      0.14      0.21        59
          11       0.27      0.15      0.19        54
          12       0.46      0.43      0.44        51
          13       0.44      0.40      0.42        50
          14       0.27      0.40      0.32        40
          15       0.40      0.44      0.42        62
          16       0.39      0.40      0.40        45
          17       0.37      0.52      0.43        44
          18       0.28      0.44      0.34        39
          19       0.38      0.28      0.32        39
          20       0.69      0.70      0.70        54
          21       0.76      0.31      0.44        42
          22       0.32      0.33      0.32        40
          23       0.68      0.47      0.56        55
          24       0.73      0.75      0.74        51
          25       0.30      0.28      0.29        58
          26       0.44      0.37      0.40        49
          27       0.34      0.22      0.27        55
          28       0.64      0.62      0.63        61
          29       0.50      0.29      0.37        58
          30       0.27      0.45      0.34        38
          31       0.35      0.49      0.41        41
          32       0.25      0.38      0.30        37
          33       0.35      0.25      0.29        53
          34       0.25      0.58      0.34        43
          35       0.23      0.21      0.22        47
          36       0.40      0.40      0.40        45
          37       0.21      0.56      0.30        61
          38       0.29      0.22      0.25        46
          39       0.55      0.48      0.51        61
          40       0.36      0.38      0.37        39
          41       0.54      0.53      0.53        53
          42       0.33      0.62      0.43        48
          43       0.51      0.50      0.51        48
          44       0.30      0.26      0.28        54
          45       0.42      0.20      0.27        49
          46       0.31      0.19      0.23        59
          47       0.74      0.40      0.52        58
          48       0.66      0.58      0.62        53
          49       0.49      0.62      0.55        53
          50       0.19      0.34      0.24        44
          51       0.51      0.37      0.43        59
          52       0.50      0.80      0.62        56
          53       0.58      0.73      0.65        60
          54       0.47      0.59      0.52        41
          55       0.21      0.11      0.15        54
          56       0.70      0.58      0.63        52
          57       0.61      0.29      0.39        59
          58       0.54      0.52      0.53        48
          59       0.50      0.22      0.31        41
          60       0.69      0.70      0.69        47
          61       0.40      0.81      0.54        52
          62       0.39      0.50      0.44        38
          63       0.37      0.46      0.41        54
          64       0.33      0.27      0.30        51
          65       0.34      0.21      0.26        53
          66       0.50      0.31      0.38        45
          67       0.31      0.28      0.30        46
          68       0.51      0.78      0.62        46
          69       0.59      0.57      0.58        40
          70       0.34      0.59      0.43        41
          71       0.53      0.55      0.54        49
          72       0.16      0.18      0.17        39
          73       0.56      0.31      0.40        45
          74       0.59      0.39      0.47        61
          75       0.70      0.63      0.67        60
          76       0.69      0.50      0.58        50
          77       0.20      0.20      0.20        51
          78       0.30      0.23      0.26        44
          79       0.40      0.20      0.27        50
          80       0.16      0.22      0.19        49
          81       0.29      0.43      0.35        47
          82       0.80      0.51      0.62        47
          83       0.30      0.45      0.36        53
          84       0.25      0.49      0.33        53
          85       0.51      0.57      0.54        49
          86       0.67      0.42      0.51        67
          87       0.62      0.64      0.63        47
          88       0.42      0.42      0.42        59
          89       0.63      0.46      0.54        41
          90       0.48      0.30      0.37        47
          91       0.53      0.47      0.50        55
          92       0.36      0.30      0.33        44
          93       0.39      0.20      0.26        45
          94       0.76      0.73      0.75        71
          95       0.57      0.41      0.48        61
          96       0.53      0.48      0.51        64
          97       0.50      0.49      0.50        61
          98       0.54      0.25      0.35        51
          99       0.30      0.23      0.26        44

    accuracy                           0.43      5000
   macro avg       0.45      0.42      0.42      5000
weighted avg       0.45      0.43      0.42      5000

** the accuracy score of current trained model on the validation dataset is : 42.58%

** the classification report of best saved model on the validation dataset is :
              precision    recall  f1-score   support

           0       0.61      0.64      0.63        42
           1       0.57      0.62      0.59        47
           2       0.33      0.25      0.29        48
           3       0.25      0.25      0.25        55
           4       0.16      0.30      0.21        46
           5       0.40      0.45      0.42        53
           6       0.59      0.50      0.54        44
           7       0.56      0.57      0.56        53
           8       0.44      0.74      0.55        35
           9       0.47      0.67      0.55        54
          10       0.47      0.29      0.36        59
          11       0.23      0.11      0.15        54
          12       0.44      0.49      0.46        51
          13       0.53      0.46      0.49        50
          14       0.41      0.38      0.39        40
          15       0.45      0.39      0.42        62
          16       0.45      0.29      0.35        45
          17       0.45      0.75      0.56        44
          18       0.34      0.41      0.37        39
          19       0.34      0.26      0.29        39
          20       0.66      0.80      0.72        54
          21       0.56      0.55      0.55        42
          22       0.26      0.35      0.30        40
          23       0.61      0.55      0.58        55
          24       0.69      0.78      0.73        51
          25       0.24      0.43      0.30        58
          26       0.37      0.47      0.41        49
          27       0.41      0.20      0.27        55
          28       0.80      0.57      0.67        61
          29       0.64      0.40      0.49        58
          30       0.28      0.42      0.34        38
          31       0.44      0.51      0.47        41
          32       0.39      0.32      0.35        37
          33       0.32      0.36      0.34        53
          34       0.44      0.40      0.41        43
          35       0.21      0.34      0.26        47
          36       0.40      0.56      0.47        45
          37       0.46      0.34      0.39        61
          38       0.38      0.20      0.26        46
          39       0.53      0.49      0.51        61
          40       0.32      0.59      0.41        39
          41       0.56      0.60      0.58        53
          42       0.44      0.58      0.50        48
          43       0.62      0.54      0.58        48
          44       0.43      0.22      0.29        54
          45       0.35      0.51      0.41        49
          46       0.27      0.37      0.31        59
          47       0.62      0.45      0.52        58
          48       0.61      0.75      0.67        53
          49       0.65      0.42      0.51        53
          50       0.22      0.25      0.23        44
          51       0.50      0.46      0.48        59
          52       0.49      0.79      0.61        56
          53       0.66      0.82      0.73        60
          54       0.54      0.46      0.50        41
          55       0.19      0.07      0.11        54
          56       0.66      0.63      0.65        52
          57       0.66      0.42      0.52        59
          58       0.48      0.54      0.51        48
          59       0.46      0.41      0.44        41
          60       0.65      0.74      0.69        47
          61       0.48      0.62      0.54        52
          62       0.50      0.26      0.34        38
          63       0.52      0.43      0.47        54
          64       0.48      0.27      0.35        51
          65       0.38      0.32      0.35        53
          66       0.52      0.49      0.51        45
          67       0.36      0.50      0.42        46
          68       0.82      0.72      0.77        46
          69       0.68      0.68      0.68        40
          70       0.57      0.56      0.57        41
          71       0.47      0.55      0.51        49
          72       0.10      0.05      0.07        39
          73       0.26      0.40      0.32        45
          74       0.42      0.41      0.42        61
          75       0.68      0.60      0.64        60
          76       0.73      0.54      0.62        50
          77       0.31      0.16      0.21        51
          78       0.22      0.23      0.22        44
          79       0.36      0.40      0.38        50
          80       0.29      0.24      0.27        49
          81       0.32      0.40      0.36        47
          82       0.70      0.81      0.75        47
          83       0.44      0.36      0.40        53
          84       0.33      0.42      0.37        53
          85       0.71      0.55      0.62        49
          86       0.58      0.57      0.57        67
          87       0.44      0.57      0.50        47
          88       0.60      0.41      0.48        59
          89       0.64      0.44      0.52        41
          90       0.41      0.34      0.37        47
          91       0.61      0.60      0.61        55
          92       0.25      0.16      0.19        44
          93       0.29      0.33      0.31        45
          94       0.69      0.83      0.76        71
          95       0.56      0.46      0.50        61
          96       0.49      0.52      0.50        64
          97       0.60      0.41      0.49        61
          98       0.34      0.39      0.37        51
          99       0.31      0.20      0.25        44

    accuracy                           0.46      5000
   macro avg       0.46      0.46      0.45      5000
weighted avg       0.47      0.46      0.46      5000

** the accuracy score of best saved model  on the validation dataset is : 46.02%

** the classification report of best saved model on the testing dataset is :
              precision    recall  f1-score   support

           0       0.61      0.64      0.63        42
           1       0.57      0.62      0.59        47
           2       0.33      0.25      0.29        48
           3       0.25      0.25      0.25        55
           4       0.16      0.30      0.21        46
           5       0.40      0.45      0.42        53
           6       0.59      0.50      0.54        44
           7       0.56      0.57      0.56        53
           8       0.44      0.74      0.55        35
           9       0.47      0.67      0.55        54
          10       0.47      0.29      0.36        59
          11       0.23      0.11      0.15        54
          12       0.44      0.49      0.46        51
          13       0.53      0.46      0.49        50
          14       0.41      0.38      0.39        40
          15       0.45      0.39      0.42        62
          16       0.45      0.29      0.35        45
          17       0.45      0.75      0.56        44
          18       0.34      0.41      0.37        39
          19       0.34      0.26      0.29        39
          20       0.66      0.80      0.72        54
          21       0.56      0.55      0.55        42
          22       0.26      0.35      0.30        40
          23       0.61      0.55      0.58        55
          24       0.69      0.78      0.73        51
          25       0.24      0.43      0.30        58
          26       0.37      0.47      0.41        49
          27       0.41      0.20      0.27        55
          28       0.80      0.57      0.67        61
          29       0.64      0.40      0.49        58
          30       0.28      0.42      0.34        38
          31       0.44      0.51      0.47        41
          32       0.39      0.32      0.35        37
          33       0.32      0.36      0.34        53
          34       0.44      0.40      0.41        43
          35       0.21      0.34      0.26        47
          36       0.40      0.56      0.47        45
          37       0.46      0.34      0.39        61
          38       0.38      0.20      0.26        46
          39       0.53      0.49      0.51        61
          40       0.32      0.59      0.41        39
          41       0.56      0.60      0.58        53
          42       0.44      0.58      0.50        48
          43       0.62      0.54      0.58        48
          44       0.43      0.22      0.29        54
          45       0.35      0.51      0.41        49
          46       0.27      0.37      0.31        59
          47       0.62      0.45      0.52        58
          48       0.61      0.75      0.67        53
          49       0.65      0.42      0.51        53
          50       0.22      0.25      0.23        44
          51       0.50      0.46      0.48        59
          52       0.49      0.79      0.61        56
          53       0.66      0.82      0.73        60
          54       0.54      0.46      0.50        41
          55       0.19      0.07      0.11        54
          56       0.66      0.63      0.65        52
          57       0.66      0.42      0.52        59
          58       0.48      0.54      0.51        48
          59       0.46      0.41      0.44        41
          60       0.65      0.74      0.69        47
          61       0.48      0.62      0.54        52
          62       0.50      0.26      0.34        38
          63       0.52      0.43      0.47        54
          64       0.48      0.27      0.35        51
          65       0.38      0.32      0.35        53
          66       0.52      0.49      0.51        45
          67       0.36      0.50      0.42        46
          68       0.82      0.72      0.77        46
          69       0.68      0.68      0.68        40
          70       0.57      0.56      0.57        41
          71       0.47      0.55      0.51        49
          72       0.10      0.05      0.07        39
          73       0.26      0.40      0.32        45
          74       0.42      0.41      0.42        61
          75       0.68      0.60      0.64        60
          76       0.73      0.54      0.62        50
          77       0.31      0.16      0.21        51
          78       0.22      0.23      0.22        44
          79       0.36      0.40      0.38        50
          80       0.29      0.24      0.27        49
          81       0.32      0.40      0.36        47
          82       0.70      0.81      0.75        47
          83       0.44      0.36      0.40        53
          84       0.33      0.42      0.37        53
          85       0.71      0.55      0.62        49
          86       0.58      0.57      0.57        67
          87       0.44      0.57      0.50        47
          88       0.60      0.41      0.48        59
          89       0.64      0.44      0.52        41
          90       0.41      0.34      0.37        47
          91       0.61      0.60      0.61        55
          92       0.25      0.16      0.19        44
          93       0.29      0.33      0.31        45
          94       0.69      0.83      0.76        71
          95       0.56      0.46      0.50        61
          96       0.49      0.52      0.50        64
          97       0.60      0.41      0.49        61
          98       0.34      0.39      0.37        51
          99       0.31      0.20      0.25        44

    accuracy                           0.46      5000
   macro avg       0.46      0.46      0.45      5000
weighted avg       0.47      0.46      0.46      5000

** the accuracy score of best saved model  on the testing dataset is : 45.99%

spent time is 0:13:56.121392
