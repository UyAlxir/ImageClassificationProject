** args :
do_train  :  True
do_valid  :  True
do_test  :  True
model  :  Conv2Layer
model_path  :  ../saved_model/
datasets_path  :  ../datasets/
dataset  :  CIFAR100
train_batch_size  :  128
test_batch_size  :  128
epoch  :  20
with_cuda  :  True
lr  :  0.1
log_path  :  ../log/
device  :  cuda:0
** args 


Conv2Layer(
  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (pool1): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(12, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (pool2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)
  (drop): Dropout(p=0.2, inplace=False)
  (fc): Linear(in_features=3072, out_features=100, bias=True)
)

** training is finished **
** the classification report of current trained model on the validation dataset is :
              precision    recall  f1-score   support

           0       0.46      0.31      0.37        42
           1       0.30      0.34      0.32        47
           2       0.18      0.12      0.15        48
           3       0.14      0.15      0.14        55
           4       0.10      0.07      0.08        46
           5       0.38      0.15      0.22        53
           6       0.28      0.30      0.29        44
           7       0.42      0.28      0.34        53
           8       0.47      0.49      0.48        35
           9       0.46      0.41      0.43        54
          10       0.24      0.29      0.26        59
          11       0.15      0.15      0.15        54
          12       0.47      0.41      0.44        51
          13       0.24      0.26      0.25        50
          14       0.27      0.23      0.25        40
          15       0.29      0.19      0.23        62
          16       0.30      0.20      0.24        45
          17       0.35      0.57      0.43        44
          18       0.18      0.23      0.20        39
          19       0.18      0.38      0.25        39
          20       0.69      0.67      0.68        54
          21       0.30      0.40      0.35        42
          22       0.27      0.33      0.30        40
          23       0.49      0.45      0.47        55
          24       0.46      0.55      0.50        51
          25       0.14      0.17      0.15        58
          26       0.23      0.29      0.26        49
          27       0.24      0.22      0.23        55
          28       0.71      0.52      0.60        61
          29       0.46      0.22      0.30        58
          30       0.54      0.18      0.27        38
          31       0.48      0.29      0.36        41
          32       0.17      0.22      0.19        37
          33       0.28      0.15      0.20        53
          34       0.19      0.23      0.21        43
          35       0.18      0.23      0.20        47
          36       0.22      0.38      0.28        45
          37       0.17      0.30      0.21        61
          38       0.20      0.20      0.20        46
          39       0.33      0.21      0.26        61
          40       0.38      0.33      0.36        39
          41       0.63      0.49      0.55        53
          42       0.37      0.33      0.35        48
          43       0.38      0.42      0.40        48
          44       0.06      0.06      0.06        54
          45       0.09      0.20      0.12        49
          46       0.18      0.31      0.23        59
          47       0.44      0.64      0.52        58
          48       0.59      0.68      0.63        53
          49       0.48      0.30      0.37        53
          50       0.06      0.09      0.07        44
          51       0.27      0.24      0.25        59
          52       0.55      0.43      0.48        56
          53       0.66      0.67      0.66        60
          54       0.29      0.27      0.28        41
          55       0.00      0.00      0.00        54
          56       0.69      0.42      0.52        52
          57       0.36      0.42      0.39        59
          58       0.34      0.50      0.41        48
          59       0.27      0.27      0.27        41
          60       0.64      0.64      0.64        47
          61       0.48      0.60      0.53        52
          62       0.37      0.37      0.37        38
          63       0.32      0.30      0.31        54
          64       0.05      0.10      0.06        51
          65       0.10      0.08      0.09        53
          66       0.18      0.20      0.19        45
          67       0.36      0.28      0.32        46
          68       0.77      0.65      0.71        46
          69       0.40      0.65      0.50        40
          70       0.27      0.34      0.30        41
          71       0.49      0.51      0.50        49
          72       0.16      0.08      0.10        39
          73       0.23      0.24      0.24        45
          74       0.32      0.18      0.23        61
          75       0.48      0.47      0.47        60
          76       0.66      0.38      0.48        50
          77       0.24      0.16      0.19        51
          78       0.16      0.16      0.16        44
          79       0.28      0.14      0.19        50
          80       0.19      0.14      0.16        49
          81       0.25      0.34      0.29        47
          82       0.68      0.60      0.64        47
          83       0.25      0.25      0.25        53
          84       0.19      0.40      0.26        53
          85       0.37      0.27      0.31        49
          86       0.40      0.34      0.37        67
          87       0.34      0.32      0.33        47
          88       0.21      0.14      0.16        59
          89       0.27      0.34      0.30        41
          90       0.20      0.28      0.23        47
          91       0.41      0.36      0.38        55
          92       0.12      0.11      0.12        44
          93       0.18      0.24      0.21        45
          94       0.83      0.68      0.74        71
          95       0.53      0.34      0.42        61
          96       0.40      0.52      0.45        64
          97       0.32      0.26      0.29        61
          98       0.27      0.14      0.18        51
          99       0.16      0.09      0.12        44

    accuracy                           0.32      5000
   macro avg       0.33      0.31      0.31      5000
weighted avg       0.34      0.32      0.32      5000

** the accuracy score of current trained model on the validation dataset is : 31.78%

** the classification report of best saved model on the validation dataset is :
              precision    recall  f1-score   support

           0       0.58      0.50      0.54        42
           1       0.36      0.51      0.42        47
           2       0.26      0.12      0.17        48
           3       0.19      0.25      0.22        55
           4       0.11      0.13      0.12        46
           5       0.37      0.28      0.32        53
           6       0.23      0.61      0.33        44
           7       0.25      0.53      0.34        53
           8       0.31      0.69      0.43        35
           9       0.46      0.48      0.47        54
          10       0.42      0.19      0.26        59
          11       0.23      0.13      0.16        54
          12       0.55      0.43      0.48        51
          13       0.41      0.40      0.40        50
          14       0.80      0.20      0.32        40
          15       0.39      0.21      0.27        62
          16       0.26      0.42      0.32        45
          17       0.49      0.43      0.46        44
          18       0.24      0.21      0.22        39
          19       0.38      0.26      0.31        39
          20       0.66      0.72      0.69        54
          21       0.41      0.43      0.42        42
          22       0.36      0.38      0.37        40
          23       0.41      0.53      0.46        55
          24       0.54      0.61      0.57        51
          25       0.28      0.17      0.21        58
          26       0.57      0.24      0.34        49
          27       0.26      0.33      0.29        55
          28       0.63      0.62      0.63        61
          29       0.55      0.29      0.38        58
          30       0.40      0.47      0.43        38
          31       0.30      0.49      0.37        41
          32       0.27      0.19      0.22        37
          33       0.18      0.36      0.24        53
          34       0.28      0.26      0.27        43
          35       0.15      0.30      0.20        47
          36       0.34      0.51      0.41        45
          37       0.31      0.30      0.30        61
          38       0.24      0.15      0.19        46
          39       0.33      0.30      0.31        61
          40       0.38      0.36      0.37        39
          41       0.74      0.43      0.55        53
          42       0.25      0.40      0.30        48
          43       0.38      0.52      0.44        48
          44       0.29      0.20      0.24        54
          45       0.30      0.06      0.10        49
          46       0.21      0.12      0.15        59
          47       0.62      0.41      0.49        58
          48       0.71      0.70      0.70        53
          49       0.50      0.43      0.46        53
          50       0.19      0.09      0.12        44
          51       0.43      0.32      0.37        59
          52       0.48      0.57      0.52        56
          53       0.55      0.78      0.65        60
          54       0.64      0.34      0.44        41
          55       0.09      0.02      0.03        54
          56       0.69      0.42      0.52        52
          57       0.53      0.34      0.41        59
          58       0.39      0.50      0.44        48
          59       0.26      0.39      0.31        41
          60       0.53      0.77      0.63        47
          61       0.50      0.54      0.52        52
          62       0.60      0.16      0.25        38
          63       0.38      0.30      0.33        54
          64       0.24      0.10      0.14        51
          65       0.16      0.17      0.17        53
          66       0.20      0.29      0.24        45
          67       0.32      0.54      0.40        46
          68       0.57      0.72      0.63        46
          69       0.70      0.53      0.60        40
          70       0.41      0.44      0.42        41
          71       0.45      0.53      0.49        49
          72       0.13      0.18      0.15        39
          73       0.26      0.22      0.24        45
          74       0.43      0.33      0.37        61
          75       0.82      0.38      0.52        60
          76       0.40      0.70      0.51        50
          77       0.25      0.24      0.24        51
          78       0.28      0.11      0.16        44
          79       0.38      0.34      0.36        50
          80       0.17      0.22      0.19        49
          81       0.25      0.28      0.26        47
          82       0.71      0.68      0.70        47
          83       0.31      0.42      0.35        53
          84       0.22      0.28      0.25        53
          85       0.45      0.61      0.52        49
          86       0.59      0.39      0.47        67
          87       0.38      0.53      0.45        47
          88       0.28      0.32      0.30        59
          89       0.45      0.22      0.30        41
          90       0.37      0.23      0.29        47
          91       0.41      0.49      0.45        55
          92       0.29      0.18      0.22        44
          93       0.37      0.22      0.28        45
          94       0.75      0.80      0.78        71
          95       0.48      0.39      0.43        61
          96       0.34      0.62      0.44        64
          97       0.33      0.30      0.31        61
          98       0.37      0.14      0.20        51
          99       0.17      0.18      0.18        44

    accuracy                           0.37      5000
   macro avg       0.39      0.37      0.36      5000
weighted avg       0.40      0.37      0.37      5000

** the accuracy score of best saved model  on the validation dataset is : 37.44%

** the classification report of best saved model on the testing dataset is :
              precision    recall  f1-score   support

           0       0.58      0.50      0.54        42
           1       0.36      0.51      0.42        47
           2       0.26      0.12      0.17        48
           3       0.19      0.25      0.22        55
           4       0.11      0.13      0.12        46
           5       0.37      0.28      0.32        53
           6       0.23      0.61      0.33        44
           7       0.25      0.53      0.34        53
           8       0.31      0.69      0.43        35
           9       0.46      0.48      0.47        54
          10       0.42      0.19      0.26        59
          11       0.23      0.13      0.16        54
          12       0.55      0.43      0.48        51
          13       0.41      0.40      0.40        50
          14       0.80      0.20      0.32        40
          15       0.39      0.21      0.27        62
          16       0.26      0.42      0.32        45
          17       0.49      0.43      0.46        44
          18       0.24      0.21      0.22        39
          19       0.38      0.26      0.31        39
          20       0.66      0.72      0.69        54
          21       0.41      0.43      0.42        42
          22       0.36      0.38      0.37        40
          23       0.41      0.53      0.46        55
          24       0.54      0.61      0.57        51
          25       0.28      0.17      0.21        58
          26       0.57      0.24      0.34        49
          27       0.26      0.33      0.29        55
          28       0.63      0.62      0.63        61
          29       0.55      0.29      0.38        58
          30       0.40      0.47      0.43        38
          31       0.30      0.49      0.37        41
          32       0.27      0.19      0.22        37
          33       0.18      0.36      0.24        53
          34       0.28      0.26      0.27        43
          35       0.15      0.30      0.20        47
          36       0.34      0.51      0.41        45
          37       0.31      0.30      0.30        61
          38       0.24      0.15      0.19        46
          39       0.33      0.30      0.31        61
          40       0.38      0.36      0.37        39
          41       0.74      0.43      0.55        53
          42       0.25      0.40      0.30        48
          43       0.38      0.52      0.44        48
          44       0.29      0.20      0.24        54
          45       0.30      0.06      0.10        49
          46       0.21      0.12      0.15        59
          47       0.62      0.41      0.49        58
          48       0.71      0.70      0.70        53
          49       0.50      0.43      0.46        53
          50       0.19      0.09      0.12        44
          51       0.43      0.32      0.37        59
          52       0.48      0.57      0.52        56
          53       0.55      0.78      0.65        60
          54       0.64      0.34      0.44        41
          55       0.09      0.02      0.03        54
          56       0.69      0.42      0.52        52
          57       0.53      0.34      0.41        59
          58       0.39      0.50      0.44        48
          59       0.26      0.39      0.31        41
          60       0.53      0.77      0.63        47
          61       0.50      0.54      0.52        52
          62       0.60      0.16      0.25        38
          63       0.38      0.30      0.33        54
          64       0.24      0.10      0.14        51
          65       0.16      0.17      0.17        53
          66       0.20      0.29      0.24        45
          67       0.32      0.54      0.40        46
          68       0.57      0.72      0.63        46
          69       0.70      0.53      0.60        40
          70       0.41      0.44      0.42        41
          71       0.45      0.53      0.49        49
          72       0.13      0.18      0.15        39
          73       0.26      0.22      0.24        45
          74       0.43      0.33      0.37        61
          75       0.82      0.38      0.52        60
          76       0.40      0.70      0.51        50
          77       0.25      0.24      0.24        51
          78       0.28      0.11      0.16        44
          79       0.38      0.34      0.36        50
          80       0.17      0.22      0.19        49
          81       0.25      0.28      0.26        47
          82       0.71      0.68      0.70        47
          83       0.31      0.42      0.35        53
          84       0.22      0.28      0.25        53
          85       0.45      0.61      0.52        49
          86       0.59      0.39      0.47        67
          87       0.38      0.53      0.45        47
          88       0.28      0.32      0.30        59
          89       0.45      0.22      0.30        41
          90       0.37      0.23      0.29        47
          91       0.41      0.49      0.45        55
          92       0.29      0.18      0.22        44
          93       0.37      0.22      0.28        45
          94       0.75      0.80      0.78        71
          95       0.48      0.39      0.43        61
          96       0.34      0.62      0.44        64
          97       0.33      0.30      0.31        61
          98       0.37      0.14      0.20        51
          99       0.17      0.18      0.18        44

    accuracy                           0.37      5000
   macro avg       0.39      0.37      0.36      5000
weighted avg       0.40      0.37      0.37      5000

** the accuracy score of best saved model  on the testing dataset is : 38.00%

spent time is 0:07:32.302949
